---
layout: post
title: Genetic algorithm for Register Allocation
subtitle: Experiment that gave great insights
tags: [register allocation, LSRA, RyuJIT]
---

# Engineering SVE in .NET

## Introduction

128-bits is 16B and 256-bits are 32B

lot of technical information along with SVE design, etc.

blog post is arranged in chronological order of implementation

## What is SVE?

SVE an acronym for Scalable Vector Extension is the Arm processor feature that lets you program on vector length that is hardware defined and developers do not have to worry about it. Once the code is written, it doesn't need be recompiled for the future version of hardware that can expand the vector length. There are numerous articles and blog posts that describes SVE. 

Explain difference between non-streaming and streaming mode SVE.

## Predicates

diagrams of predicate register and some operations



## Backend

To include an ISA in a compiler, there is a pre-requisite to add support for the new instructions the ISA offers, so that compiler can generate them. There are lot of details that goes in for implementing instructions. Starting with the lowest level, the opcodes of instrutions need to be added in the codebase, along with the corresponding pneumonic names. Many instructions comes with different versions having different number of operands, size of data they handle or various register requirements. All these finer details need to be fed in an easy to understand lookup table. Next, the code generation infrastructure need to be updated to make sure that the new instructions can be emitted in relevant places. Higher up, if there are any new registers introduced as per of the instruction, they need to be added in the register allocation. Every ISA defines its own Application Binary Interface (ABI) that specifies what happens to the state of registers at the sub-routine boundary and whose responsibility it is to save and restore the registers. In below sections, we will look through the work we have done in each of these phases to support SVE.

### SVE encoding

In compiler literature, "instruction encodes" refers to the opcodes of each instruction that are found in the Architecture manual. Compilers usually embedded these opcodes in some kind of lookup table such that key is a pneumonic name (or instruction name that is easy to understand by the developer) and value is the opcode of that instruction. The code generator knows which instruction to emit and it invokes the encoder to output the corresponding opcodes in the final code buffer.

![alt text](image-4.png)

In above screenshot, taken from Arm manual, the opcode of `setffr` instruction can be seen as `0x252C9000`. 

Now, lets look at step by step on how the opcodes that are found in the instruction manual get it through the compiler codebase.

1. The opcodes of new instructions has to be added in our codebase, such that the encoder and code generator has access to it.

TODO-START: Do not include
 This can be seen in [instrsarm64sve.h](TODO) file as shown below.

<< TODO: snapshot of instrarm64sve.h for SETFFR>>

Next, there are several code paths that need to be updated in order to enable the instruction that got added in above lookup table.

TODO-END: Do not include

2. For a code generator, to emit the any instruction, there is a giant `switch-case` against the instruction to be emitted. Depending on the instruction, the individual `case` statement calls the encoder to perform the encoding. The new instruction need to be added in such `switch-case`.

3. If an instruction has multiple formats, we need to update the code paths, to include the formats of the newly added instruction. 

4. There are sanity checks that are done while emitting each instruction to make sure that the instruction we are emitting has right set of registers, the data/vector size embedded in the opcode of the instruction is relevant, the immediates present in the instruction is within the allowable range, etc. All these codepaths need to get updated for any new instruction we add. 

5. For a disassembler, to display the pneumonic names of the instruction for a developer, we need to update the codepaths to include the new instruction to display them appropriately.

6. If we want to calculate the performance characteristics like latency and throughput of the newly added instruction, we need to update those paths.

7. Lastly, if there is a provision to unit test the instruction, we need to add unit tests for the new instructions.

What we can infer is for every new instruction a compiler adds support for, there are handful of places that needs to be updated to make sure the new instruction is completely enabled and functional. For SVE, we had to add 1123 new instructions. Yes, that is "One Thousand, One Hundred and Twenty Three" instructions. For an engineer to add support for a single instruction, from inspecting the opcodes in the Arm Manual, to understand the opcodes, to typing in the opcodes, to updating all the code paths, and all the places I described above, up to the point they can validate the instruction with unit test, it will at least take 3~4 hours to complete the task. For 1123 instructions, that is 561 work days, or little over 2 years of work for a single engineer.

With those timelines and estimate, there was no one we could have finished adding all the instructions, let alone exposing the intrinsics in .NET APIs layer. We realized that most of the code I mentioned above thatw was required to light up an instruction was very similar. All it needed was to translate an instruction details presented in the Arm architecture manual into the C++ code that can fit in RyuJIT's codebase. We started exploring options of auto generating some portion of the C++ code. To do that, we needed some utility that will scan pdf documents (Arm manual that is in pdf format) and extract out important information from it. However, we did not have such tool at our exposure or expertise to write a custom utility tool that knows how to parse the pdf pages. 

#### SVE encoding auto-generated C++ code

Arm Ltd. had [xml version](https://developer.arm.com/Architectures/A-Profile%20Architecture#Software-Download) of all their instruction set, which was much easier to write a parser for. We spent 3~4 weeks to write a tool in C# (of course) that parsed the xml files containing instruction data and generating various versions of C++ code that was needed at several places in our codebase. If you see [SveEncodingFiles](https://github.com/kunalspathak/SveEncodingFiles), the tool produced more than 17K lines of C++ code spread across 15 files. Imagine what a hard task it would be for an engineer having to type all this code by hand! The tool proved to be a great value add for our team and was worth pursuing.

When it came to implementation, all that an engineer had to do was copy the auto-generated code for each instruction from various files and paste it at the right place in our codebase. We distributed the instructions among 3 engineers. As you must have guessed, we also auto-generated the [distribution of instructions](https://github.com/kunalspathak/SveEncodingFiles/blob/main/assignments.md) among engineers. The engineers were able to implement all the instructions without having to worry about ay dependencies on each other or complicated merge conflicts during development. We finished adding all the 1123 encodings in little over 3 months. Hand writing the code for encoding would have took almost 9 months for 3 engineers. That was a total saving of 6 months for 3 engineers, or 18 man-months!

While there were lot of challenges in understanding the xml files semantics, how they are interconnected to other instruction files and writing parsing tools to make sure that we do not interpret the data incorrectly, it is worth its own blog post. Here, I will just walk you through few auto-generated files to highlight some of the key design of the tool.

1. [instrsarm64sve.h](https://github.com/kunalspathak/SveEncodingFiles/blob/main/instrsarm64_sve.h): All the instructions that RyuJIT offers are stored in a table. Each instruction can have several different formats by which they operate, depending on the size of vector, size of data, size of individual elements, those containing immediate operand and so forth. For e.g., `ld1sb` instruction's entry in `Fig 1.` has 6 different formats (as seen in [1](https://docsmirror.github.io/A64/2023-06/ld1sb_z_p_bi.html), [2](https://docsmirror.github.io/A64/2023-06/ld1sb_z_p_br.html), [3](https://docsmirror.github.io/A64/2023-06/ld1sb_z_p_bz.html) and [4](https://docsmirror.github.io/A64/2023-06/ld1sb_z_p_ai.html)). Every instruction format has the normalized opcodes in binary and hexadecimal formats. In the first instruction format of `Fig. 1`, the binary representation comes from [the Arm manual](https://docsmirror.github.io/A64/2023-06/ld1b_z_p_bz.html) seen in `Fig 2.`. The normalized version in our codebase basically has `0` at all the bit positions that encodes the information during code generation. One of the example is registers used in an instruction. If you see the binary representation, there are characters like `g`, `n` and `t` in them. Those positions are filled up when register corresponding to `g`, `n` and `t` are known during code generation. Without the tool, it would have been very time consuming task to translate the information in Arm manual (from `Fig 2.`) into the C++ code (in `Fig 1.`). 

   Other critical information that the tool helped generate for us was the format names. In `Fig 1.`, all instruction formats have names that starts with `SVE_*`. All instruction formats (of different instructions) that share same "normalized" binary/hexadecimal encoding gets the same format name. The rational behind this design is that all the formats having same "normalized" encoding, would get encoded during code generation in similar way. By grouping all such instruction formats with a format name that share similar encoding logic, we easily can share the C++ code needed to handle those formats (as you will see further). Now, imagine if each engineer start implementing the instructions assigned to them, it will be very challenging and difficult to co-ordinate to see if the format they are working on already exists and if not, the nomenclature they should use for the format name. By letting the tool do the heavy lifting for us, we embedded the logic of grouping various instruction formats and naming them with consistent naming conventions in the tool itself. As a result, the engineer was not burdened with this thinking of grouping similar formats.


   ![alt text](image-2.png)

   <p style="text-align: center;">Fig 1. Instruction encoding formats for "ld1sb"</p>

   ![alt text](image-3.png)

   <p style="text-align: center;">Fig 2. Arm manual's encoding entry for "ld1sb"</p>

2. [emitfmtsarm64sve.h](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitfmtsarm64_sve.h): Talking about the instruction format names, we also have a lookup table for individual format names along with the format it represents and brief description. From the Arm manual, we were able to easily extract this information and generate the lookup table with the new SVE format names. The nomenclature that we came up with was `SVE_XX_AB`, where `XX` is just alphabetical in order starting with `AA`, `AB` and so forth. `A` represents the number of register the format operates on, and `B` is another alphabetical order if there are slight variations from `SVE_XX_A` format. 

   ![alt text](image-5.png)


   <p style="text-align: center;">Fig 3. List of instruction format names</p>


3. Instruction to format mapping ([1](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_I_sve.cpp), [2](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_I_sve.cpp), [3](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_R_I_sve.cpp), [4](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_R_R_I_sve.cpp), [5](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_R_R_sve.cpp), [6](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_R_sve.cpp), [7](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_R_sve.cpp), [8](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitIns_R_sve.cpp)): Since the auto generator tool already had the mapping of instruction format names to the instruction, we also leveraged it to generate reverse mapping needed to select the instruction format, depending on the instruction we want to emit.

   ![alt text](image-10.png)
    <p style="text-align: center;">Fig TODO. Instruction -> format mapping</p>

3. [emitOutputInstr.cpp](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitOutputInstr_sve.cpp), [dispHelper.cpp](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitDispInsHelp_sve.cpp), [sanityCheck.cpp](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitInsSanityCheck_sve.cpp): The next important piece of code the tool helped us generate was a huge switch/case, where depending on individual instruction format names, there is a common logic of handling them at various places in our codebase.

   In `Fig 4.` below, there is an encoding logic for `IF_SVE_BJ_2A` and following cases. It starts with the encodings we have in `instrsarm64sve.h`. Then, we encode the register represented by `nnnnn`, followed by the register represented by `ddddd` and then the element size `xx` of the instruction we are emitting. Because of the virtue of having all the manual's data handy, the tool was able to generate nice comments and other finer details that are easy to miss, if done manually.

   ![alt text](image-6.png)
   <p style="text-align: center;">Fig 4. Code to encoding instruction format</p>

   In a similar fashion, the tool generated all the relevant code necessary to display the disassembly of each instruction as seen in `Fig 5.` below. 
   To verify the encodings and the disassembly that we were generating is correct, we wanted an external tool that will take the encodes as input and produce the disassembly code. We would then compare the disassembly we produce with that of the external tool. One of our engineer forked [capstone](https://github.com/TIHan/capstone/tree/capstone-jit2) to make it work with RyuJIT. Each time an engineer would implement set of instructions, we would ask them to compare the disassembly RyuJIT produced with capstone's disassembly as seen in one of [this PR](https://github.com/dotnet/runtime/pull/95679).

   ![alt text](image-7.png)
    <p style="text-align: center;">Fig 5. Code to display disassembly</p>

   Lastly, to ensure that the instruction contains correct information like register, data size, etc. embedded, we do a round of sanity check and new checks were added by the tool for the SVE instructions as seen below.

   ![alt text](image-8.png)
    <p style="text-align: center;">Fig 6. Code to perform sanity checks</p>


4. [PerfScore.cpp](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitPerfScore_sve.cpp): For our debugging purpose, we emit the latency and throughput information of the entire method to understand how fast or slow the method would be. The latency and throughput information of individual instructions is taken from the Architecture Manuals and is embedded in our codebase. For new instructions, it would have been again tedious to find it in the manual and put it in our codebase. The tool helped to come up with
[perfscore.md](https://github.com/kunalspathak/SveEncodingFiles/blob/main/perfscore.md) that contained all the information needed for this purpose. As seen in `Fig todo.`, depending on instruction format and specific instruction, we embedded the perfscore information in our codebase.


   ![alt text](image-11.png)
   <p style="text-align: center;">Fig todo. Perfscore calculations</p>

6. [UnitTests.cpp](https://github.com/kunalspathak/SveEncodingFiles/blob/main/emitArm64EmitterUnitTests_sve.cpp): Finally, any feature is not complete without adequant test coverage. We were able to auto-generate unit tests to invoke all the above code as seen in `Fig todo.`

   ![alt text](image-13.png)
   <p style="text-align: center;">Fig todo. Unit tests for instructions</p>

To read more details and our progress, you can refer to the [dotnet/runtime #94549](https://github.com/dotnet/runtime/issues/94549) that lists all the encodings we added as well as the links to all the PRs that implemented 1123 instructions.

![alt text](image-12.png)
<p style="text-align: center;">Fig todo. Sve encodings progress</p>

### Handle more than 64 registers

The next important topic to talk about is having the support in register allocator. Prior to SVE, AArch64 had 64 registers - 32 general purpose registers (GPR) and 32 SIMD/Floating-Point registers. Throughout the RyuJIT backend code, we handle register set in lot of places. We represent them in a bitmask of type `unsigned __int64` and named it as `regMaskTP`. By treating register set as bitmask made it simpler to perform queries and operations like "Is a register present in the set?" or "Remove a register from the set" and so on. We also need to keep track of registers that are free/busy at a given point for which bitmask is useful. There are also numerous methods in our backend that either takes the `regMaskTP` as parameter (to know which registers are involved in an operation) or returns `regMaskTP` (to return the information about number of registers involved). Lastly, we had these register sets as part of our Intermediate Representation (IR) tree nodes. For such purpose too, `regMaskTP` has proven useful so far. 

However, as I have briefly mentioned above, SVE instruction set introduces 16 new predicate registers. That makes the total count of AArch64 registers to 80, which made it impossible to use our `regMaskTP` data structure. The obvious solution for such problem was to convert the `regMaskTP` into a struct and have additional field for representing the new register set. Our [quick prototype revealed](https://github.com/dotnet/runtime/pull/96196#issuecomment-1864657071) that by doing it increases the throughput cost of JIT compilation by around 10%. The reason being the native code produced for the RyuJIT code was sub-optimal because of replacing primitive `unsigned __int64` with a `struct`. All the methods that took or returned `regMaskTP` got impacted because the native code produced for such methods emitted additional code to copy around one extra field, often leading to disabling certain optimizations by the native C++ compiler.

Knowing that `struct` has significant overhead, prototype# 2 was to try to update the `regMaskTP` to `unsigned __int128` instead. However, we quickly realized that this option will not work. If we changed `regMaskTP` to `unsigned __int128`, all the data structure containing `regMaskTP` field assumed that the field's memory is aligned at 16-byte boundary. The native compiler would fail if that is not the case as we [found it here](https://github.com/dotnet/runtime/pull/94589#issuecomment-1816836145). This option was discarded.

Whatever prototype we try, we knew that it would impact many places in RyuJIT and we wanted to make sure that we not only minimize that impact for AArch64, but have zero impact for other platforms. To try out something that would pay off in a long run for all platforms, we had prototype# 3. We audited almost 1000s of places where `regMaskTP` was passed around. We segregated the places that are guaranteed to take just GPRs, just SIMD/Floating-Point registers, either of them, or any of them. By doing this analysis led us explore the code places where the new "predicate registers" need to be part of the register set and have the new "struct" version of `regMaskTP` only for such cases to reduce the throughput impact that we noticed in prototype# 1. We tried lot of novel techniques to reduce the throughput impact as much as possible, that an interested reader can read [here](https://github.com/dotnet/runtime/pull/98258). This prototype just had the throughput impact of 5~6% compared to that of 10% we tried earlier. However, we did not find this option viable because it touched at too many places and there were high chances that it could introduce bugs.

Knowing that the last prototype gave tolerable throughput impact, we decided to go forward with declaring `regMaskTP` as `struct` with 2 fields of `unsigned __int64` each. The first field will continue to represent GPR + SIMD/Floating-Point registers jointly, while the second field would represent the new "predicate registers". By having that separation, most of the code paths work on first field and very few places (like in register allocator itself), we had to deal with both the fields. We also decided to break down [prototype# 3](https://github.com/dotnet/runtime/pull/98258) into smaller incremental PRs, that are easy to review and thus, understand the impact of each smaller changes.

- [Handle more than 64 registers - Part 1](https://github.com/dotnet/runtime/pull/101950)
- [Handle more than 64 registers - Part 2](https://github.com/dotnet/runtime/pull/102297)
- [Handle more than 64 registers - Part 3](https://github.com/dotnet/runtime/pull/102592)
- [Handle more than 64 registers - Part 4](https://github.com/dotnet/runtime/pull/102921)
- [Handle more than 64 registers - Part 5](https://github.com/dotnet/runtime/pull/103188)
- [Handle more than 64 registers - Part 6](https://github.com/dotnet/runtime/pull/103387)


While register allocator support was getting added, we wanted to make sure that our engineers implementing the instruction encoding are not blocked because of lack of predicate registers. As such, we set the predicate registers to be an alias of vector registers. For e.g. `P0` was set to be alias of `V0`, `P1` to be alias of `V1` and so forth. That enabled our engineers to use enums like `REG_P0`, `REG_P1`, etc. in instruction encoding.

Overall, it took us several months to get these changes in, but it proved to be very useful infrastructure work to have. In future release, .NET is planning to add support for [Intel APX](https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html) which too adds additional GPR registers and will surpass the number of registers beyond 64. For lighting up new registers for Intel APX, very minimal change will be needed on top of the support we added in register allocator to support more than 64 registers.

### Calling conventions

With the completion of predicate register support in our register allocator, we started to represent these registers as its own dedicated entities, similar to GPR and SIMD/Floating-Point registers. The next logical thing to do was to implement the calling conventions defined for SVE. To revise the concept, lets say a method `A` (the caller) calls method `B` (the callee). Out of the other rules, an important rule that the calling convention describes is which registers should be preserved by callee and caller such that after the method call is complete, the caller can proceed its execution. Imagine the calling convention states that registers `r5` and `r6` should be preserved by the caller, while `r7` and `r8` should be preserved by the callee. We refer `r5` and `r6` as callee-trash/caller-saved, because callee can freely overwrite its contents without backing up its contents. Hence it is caller's responsibility to preserve its contents, if they are holding a value of a live variable. Likewise, `r7` and `r8` are referred as callee-saved registers because it is callee's responsibility to save/restore them (in prolog/epilog), if it want to use it.


```asm
func A()
{
   r5 = ...
   r6 = ...
   r8 = ...

   save r5
   B();
   restore r5

   ... = r5
   ... = r8
}

func B()
{
   push r8 ; prolog

   r8 = ...
   r5 = ...
   ...
   ...= r8


   pop r8 ; epilog
}
```

In code snippet above, although `r5`/`r6` are callee-trash registers, only `r5` is saved in `A` and restored after the call to `B` because there is no use of `r6` after `B` and hence there is no need to save/restore it. Likewise, in `B`, although `r8`/`r9` are callee-saved registers, only `r8` is saved in prolog and restored in epilog because `r9` was not touched through out the method. Once the call to `B` is complete, the value of `r5`, that was overwritten in `B`, is restored back to the value that was prior to `B` (hence `r5` is referenced as caller-saved). Likewise, `r8`'s value was restored, but the difference being the restoration of `r8` was done by callee (hence `r8` is referenced as callee-saved).


Here are the extracts about SVE calling convention as stated in the Arm architecture manual:

[Scalable Vector Registers](https://github.com/ARM-software/abi-aa/blob/main/aapcs64/aapcs64.rst#613scalable-vector-registers):

```
z0-z7 are used to pass scalable vector arguments to a subroutine, and to return scalable vector results from a function. If a subroutine takes at least one argument in scalable vector registers or scalable predicate registers, or returns results in such regisers, the subroutine must ensure that the entire contents of z8-z23 are preserved across the call. In other cases it need only preserve the low 64 bits of z8-z15.
```

[Scalable Predicate Registers](https://github.com/ARM-software/abi-aa/blob/main/aapcs64/aapcs64.rst#scalable-predicate-registers):

```
p0-p3 are used to pass scalable predicate arguments to a subroutine and to return scalable predicate results from a function. If a subroutine takes at least one argument in scalable vector registers or scalable predicate registers, or returns results in such registers, the subroutine must ensure that p4-p15 are preserved across the call. In other cases it need not preserve any scalable predicate register contents. In other cases it need not preserve any scalable predicate register contents.
```

After a [long discussion](https://github.com/ARM-software/abi-aa/issues/266) with Arm manual writers, we got clarifications about this requirement, which can be summarized in below table:

|Callee type | Callee saves | Caller saves
|---|---|---|
| regular | bottom 64-bits v8-v15 | All registers not in bottom 64-bits `v8-v15`
| *sve | `z8-z23`, `p4-p15` | All registers not in {`z8-z23, p4-p15`}

*sve method is defined as having at least one parameter as scalable vector/predicate value or that returns such value.

In other words, the calling convention mentions to save/restore different set of vector and predicate registers, depending on the method being called is a sve or a regular method. There were few challenges with having this convention implemented in .NET. The easier and solvable problem was that during jitting a method, we would now have to track if the current method is a sve method or if there is a call to a sve method. However, the challenging problem was the produced JIT code calls into native helpers, which are usually regular methods since we have not used scalable vector/predicate values in its parameters/return. Lets look again at the table above to understand the calling convention for `sve -> regular` call scenario.
- The caller (`sve` method) saves/restore {`z8-z23`, `p4-p15`} in its prolog/epilog because callee (`regular` method just saves bottom `v8-v15` and hence others can get overwritten).
- The callee (`regular` method) saves/restore {bottom 64-bits `v8-v15`} in prolog/epilog. Everything else should be saved/restored by the caller.

To visualize the disassembly produced by GCC/LLVM with all the saves/restore done correctly, lets look at two simple C++ programs. Both `M1` and `M2` are sve methods, however the difference is `M1` operates on scalable values (`pg`, `x` and `y`) while `M2` does not. `M2` just takes scalable values as parameters.

```c++
extern void N();
extern svbool_t Foo();

svbool_t M1(svint8_t x, svint8_t y)
{
    svbool_t pg = Foo();
    N();
    return svcmpgt_s8(pg, x, y);
}

int z;
void M2(svint8_t x, svint8_t y)
{
    z = 500;
    printf("hello");
}
```

If we look at the optimized disassembly (using `-O3`) produced for both these methods [at godbolt](https://godbolt.org/z/8sdbo5a9x) by GCC/Clang,  we notice that there are lot of saves/restores generated. That confirms that calling a regular method from a sve method can prove expensive. So the JIT code for a sve method that calls our helpers can thus be expensive and would regress the performance.

Hence, to keep things simpler, we decided to continue follow NEON calling conventions. Row 1 "regular callee type" in above table. With that design, it was much easier to just treat all the predicate registers `p0-p15` as [callee-trash](https://github.com/dotnet/runtime/pull/104065/files#diff-9fccee792bd328bbce1878bb5f0c4b999a251cb34a6f2a4b34ccd98be16d30a7R83). Essentially, whenever we do a method call and there is a live variable value inside the predicate register, we would just save that register contents on the stack before the call and then restore it after the call.

To save and restore predicate registers on the stack, regular or NEON instructions cannot be used. Predicate registers can only be saved/restore from the stack using predicate version of [str](https://docsmirror.github.io/A64/2023-06/str_p_bi.html) / [ldr](https://docsmirror.github.io/A64/2023-06/ldr_p_bi.html) instructions respectively. This was easily done by detecting if the register involved is predicate, and if so, use the predicate version of the instructions. In .NET 9, we hardcoded the vector length (VL) to 128-bits (more on that later), and hence, we did not have to worry about the position in the stack frame layout where the values of predicate registers will be saved. Likewise, the scalable vector registers are hardcoded to be of size 128-bits, and so, we did not have to update the `str`/`ldr` instructions that are used to save and restore scalable vector register values from/to stack. Existing NEON instructions just worked for this purpose. As seen in [dotnet/runtime #104065](https://github.com/dotnet/runtime/pull/104065), only change we had to make is making predicate register part of callee-trash set and then handling the saving and restoring of those registers.

## Frontend

Now that we have seen the code generation details above, let us deep dive in the frontend side of things. In this section, I will talk through the .NET API surface we exposed for various SVE intrinsics, data type we picked to represent scalable vector and predicate values and our testing methodology. I will also touch base upon various design approaches we undertook to make the connection between .NET API layer to the SVE instruction seemless.

### (Almost) Scalable Vector\<T>

When we first decide to support SVE in .NET, one of the primary thing that we were brainstorming about was how to represent scalable register concept. All the data types that are available currently in .NET ecosystem can hold a fixed amount of data that is known ahead of time. However, the "scalable" concept of SVE enforce the compiler developers to pick a data type that is designed to have unknown size until the  runtime. `Vector<T>` is such a data type that was designed with such a scenario in mind. It represents a single vector holding X number of bits. The number of bits 

elements that can be present in `Vector<T>` is called vector length depends on the 
that 

`struct {}`

One of the primary task
[128-bit vector length](https://github.com/dotnet/runtime/pull/104174)

Issue about support VL agnostic
[dotnet/runtime #101477](https://github.com/dotnet/runtime/issues/101477)


TODO: include the issues

### SVE APIs

auto-generated code

### VectorToMask and MaskToVector conversions

### Conditional Select

###  RMW Delay free registers

### movprfx instruction

### Predicated, Unpredicated, both

## FFR register

## Diagnostics

## Context switches

## Debugging

## Testing 

unit test

stress test running locally

## Partners

Acknowledgement:
 - Alan Haywards
 - Aman Asif Khalid
 - Kunal Pathak
 - Sebastin
 - Arm engineer who was present for API implementation??
 - Swapnil
 - Will Smith

## Hardware availability

Currently SVE is available in very limited hardwares, but vendors are pushing towards introducing SVE. Microsoft Azure recently released Arm offering of Cobalt 100 that has non-streaming SVE feature. Amazon's AWS has Graviton 3 that comes with 2 x 16B VL, Graviron 4 comes with 16B vector length, Apple's M4 has 64B vector length and supports streaming SVE and SME features. And there is Fijitsu's super computer that offers SVE 64B VL.

- unwind code not available
- OS had support in January
- windbg no support

## NativeAOT

### Future

- streaming SVE and SME
- True VL agnostic


### References

- https://developer.arm.com/Architectures/A-Profile%20Architecture#Software-Download

good references that explain SVE features in details along with the coding examples.